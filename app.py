import pandas as pd
import numpy as np
import torch
import os
import faiss
import streamlit as st
from FlagEmbedding import BGEM3FlagModel
from openai import OpenAI

st.set_page_config(
    page_title="LEGAL MIND",
    page_icon="‚öñÔ∏è",
    layout="wide"
)


st.markdown("""
<div style="background-color: #FF4B4B; padding: 1.5rem; border-radius: 0.5rem; margin-bottom: 1rem;">
    <h1 style="color: white; margin: 0;">LEGAL MIND ‚öñÔ∏è</h1>
    <p style="color: white; margin-top: 0.5rem;">H·ªá th·ªëng T√¨m ki·∫øm v√† H·ªèi ƒë√°p Ph√°p lu·∫≠t</p>
</div>
""", unsafe_allow_html=True)


TOGETHER_API_KEY = 'API_KEY'

@st.cache_resource
def load_model():
    """Load the embedding model with caching to avoid reloading on each interaction"""
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cuda":
        os.environ["CUDA_VISIBLE_DEVICES"] = "0"
    
    model = BGEM3FlagModel('AITeamVN/Vietnamese_Embedding', use_fp16=True)
    return model

@st.cache_resource
def load_data():
    df = pd.read_csv('corpus.csv')
    df_embeddings = np.load("embedded_bge_train_law.npz")
    
    embeddings_array = df_embeddings['embeddings'].astype('float32')
    cid_map = df_embeddings['cid'].tolist()
    
    embeddings_array = np.ascontiguousarray(embeddings_array)

    dimension = embeddings_array.shape[1]

    index = faiss.IndexFlatL2(dimension)

    index.add(embeddings_array)
    
    return df, index

def get_query_embedding(query, model):
    embedding = model.encode(
        [query],
        batch_size=32,
        max_length=512
    )['dense_vecs']
    embedding = embedding[0]
    return np.array(embedding, dtype=np.float32)

def retrieve_documents(query, index, df, k=10):
    """Retrieve top-k documents based on FAISS similarity search"""
    model = load_model()
    
    query_embedding = get_query_embedding(query, model)
    
    if query_embedding.ndim == 1:
        query_embedding = query_embedding.reshape(1, -1)

    D, I = index.search(query_embedding, k)
    
    if len(I[0]) == 0:
        st.error("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu n√†o ph√π h·ª£p v·ªõi truy v·∫•n c·ªßa b·∫°n.")
        return []

    documents = df["text"].tolist()
    results = []
    for idx, doc_idx in enumerate(I[0]):
        results.append({
            "rank": idx + 1,
            "distance": D[0][idx],
            "text": documents[doc_idx]
        })
    
    return results

def ask_llm(query, context=""):
    """Ask a question to the LLM using Together API"""
    try:
        client = OpenAI(api_key=TOGETHER_API_KEY, base_url="https://api.together.xyz/v1")
        
        # If we have context from retrieved documents, include it
        if context:
            system_prompt = (
                "B·∫°n l√† m·ªôt ng∆∞·ªùi hi·ªÉu r·∫•t r√µ v·ªÅ lu·∫≠t ph√°p c·ªßa Vi·ªát Nam. "
                "H√£y d·ª±a v√†o th√¥ng tin ƒë∆∞·ª£c cung c·∫•p ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi. "
                "N·∫øu kh√¥ng c√≥ th√¥ng tin ƒë·ªß ƒë·ªÉ tr·∫£ l·ªùi, h√£y n√≥i r√µ ƒëi·ªÅu ƒë√≥."
            )
            user_prompt = f"Th√¥ng tin tham kh·∫£o:\n{context}\n\nC√¢u h·ªèi: {query}"
        else:
            system_prompt = "B·∫°n l√† m·ªôt ng∆∞·ªùi hi·ªÉu r·∫•t r√µ v·ªÅ lu·∫≠t ph√°p c·ªßa Vi·ªát Nam"
            user_prompt = query
        
        response = client.chat.completions.create(
            model="deepseek-ai/DeepSeek-V3",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            stream=False
        )
        
        return response.choices[0].message.content
    except Exception as e:
        return f"ƒê√£ x·∫£y ra l·ªói khi g·ªçi API: {str(e)}"

# Main application
def main():
    with st.spinner("ƒêang t·∫£i d·ªØ li·ªáu v√† m√¥ h√¨nh..."):
        df, index = load_data()
    
    st.sidebar.title("T√πy ch·ªçn")
    
    app_mode = st.sidebar.radio(
        "Ch·ªçn ch·∫ø ƒë·ªô:",
        ["T√¨m ki·∫øm t√†i li·ªáu", "H·ªèi ƒë√°p lu·∫≠t ph√°p", "T√¨m ki·∫øm + H·ªèi ƒë√°p"]
    )
    
    if app_mode == "T√¨m ki·∫øm t√†i li·ªáu":
        search_documents_ui(df, index)
    elif app_mode == "H·ªèi ƒë√°p lu·∫≠t ph√°p":
        qa_ui()
    else:
        rag_ui(df, index)

    st.sidebar.markdown("---")
    st.sidebar.markdown("- H·ªèi ƒë√°p: deepseek-ai/DeepSeek-V3")


def search_documents_ui(df, index):
    """UI for document search"""
    st.subheader("üîç T√¨m ki·∫øm t√†i li·ªáu")
    
    k = st.slider("S·ªë l∆∞·ª£ng k·∫øt qu·∫£ hi·ªÉn th·ªã", min_value=1, max_value=20, value=10)
    query = st.text_input("Nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm:")
    
    if st.button("T√¨m ki·∫øm", type="primary"):
        if query:
            with st.spinner("ƒêang t√¨m ki·∫øm t√†i li·ªáu li√™n quan..."):
                results = retrieve_documents(query, index, df, k=k)
            
            if results:
                st.success(f"T√¨m th·∫•y {len(results)} k·∫øt qu·∫£ cho: '{query}'")
                
                for result in results:
                    with st.expander(f"üìÑ T√†i li·ªáu {result['rank']} (ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {1/(1+result['distance']):.4f})"):
                        st.markdown(result['text'])
            else:
                st.info("Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ n√†o. Vui l√≤ng th·ª≠ l·∫°i v·ªõi t·ª´ kh√≥a kh√°c.")
        else:
            st.warning("Vui l√≤ng nh·∫≠p t·ª´ kh√≥a t√¨m ki·∫øm.")

def qa_ui():
    st.subheader("‚ùì H·ªèi ƒë√°p ph√°p lu·∫≠t")
    
    query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")
    
    if st.button("G·ª≠i c√¢u h·ªèi", type="primary"):
        if query:
            with st.spinner("ƒêang x·ª≠ l√Ω c√¢u h·ªèi c·ªßa b·∫°n..."):
                answer = ask_llm(query)
            
            st.subheader("C√¢u tr·∫£ l·ªùi:")
            st.markdown(answer)
        else:
            st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")

def rag_ui(df, index):
    st.subheader("üîç‚ùì T√¨m ki·∫øm v√† H·ªèi ƒë√°p")
    
    query = st.text_input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n:")
    k = st.slider("S·ªë l∆∞·ª£ng t√†i li·ªáu tham kh·∫£o", min_value=1, max_value=10, value=3)
    
    if st.button("G·ª≠i c√¢u h·ªèi", type="primary"):
        if query:
            with st.spinner("ƒêang t√¨m ki·∫øm t√†i li·ªáu li√™n quan..."):
                results = retrieve_documents(query, index, df, k=k)
            
            if results:
                context = "\n\n".join([f"T√†i li·ªáu {result['rank']}: {result['text']}" for result in results])
                
                with st.spinner("ƒêang x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi..."):
                    answer = ask_llm(query, context)
                
                st.subheader("C√¢u tr·∫£ l·ªùi:")
                st.markdown(answer)
                
                with st.expander("Xem t√†i li·ªáu tham kh·∫£o"):
                    for result in results:
                        st.markdown(f"**T√†i li·ªáu {result['rank']}** (ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng: {1/(1+result['distance']):.4f})")
                        st.markdown(result['text'])
                        st.markdown("---")
            else:
                with st.spinner("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan. ƒêang x·ª≠ l√Ω c√¢u h·ªèi tr·ª±c ti·∫øp..."):
                    answer = ask_llm(query)
                
                st.subheader("C√¢u tr·∫£ l·ªùi:")
                st.markdown(answer)
                st.info("Kh√¥ng t√¨m th·∫•y t√†i li·ªáu li√™n quan. C√¢u tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c chung c·ªßa m√¥ h√¨nh.")
        else:
            st.warning("Vui l√≤ng nh·∫≠p c√¢u h·ªèi.")

if __name__ == "__main__":
    main()
